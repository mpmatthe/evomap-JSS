
\section{Detailed Usage Example} \label{sec:usage-example}

This section illustrates the practical application of the evomap package using a small sample from the
TNIC data \cite{Hoberg+Phillips:2016} provide. For each step in the analysis, this section details which
tools the \pkg{evomap} package provides to support its user and illustrate their usage with the TNIC sample
data.

The TNIC sample data comprise a network of firm-firm relationships derived from linguistic
similarities among the product descriptions in their annual reports. The sample data used herein
includes nine technology firms: \emph{Apple, AT\&T, eBay, Intuit, Micron Technology, Microsoft,
Oracle, US Cellular, and Western Digital}. The sample data include observations once per year
throughout 20 subsequent years between 1998 and 2017. Table 3-2 presents the first and last five
rows of the data, where each observation represents a firm-firm pair at a specific point in time. The
‚Äúname1‚Äù/‚Äùname2‚Äù columns indicate the firms‚Äô names, the ‚Äúscore‚Äù column indicates their pairwise
similarity, while the ‚Äúsic1‚Äù/‚Äùsic2‚Äù columns indicate the firms‚Äô SIC industry codes, and the
‚Äúsize1‚Äù/‚Äùsize2‚Äù columns represent a synthetic variable based on a monotonic transformation of their
market values. We can load the TNIC sample data from the datasets submodule:

\begin{Code}
>>> from evomap.datasets import load_tnic_sample_tech
>>> data = load_tnic_sample_tech()
\end{Code}
  
\begin{table}[t!]
  \centering
  \begin{tabular}{ccp{3cm}p{3cm}cccccc}
  \hline
  row & year & name1 & name2 & score & sic1 & sic2 & size1 & size2 \\ 
  \hline
  0 & 1998 & APPLE INC & WESTERN DIGITAL CORP & 0.0657 & 36 & 35 & 71.79 & 32.29 \\
  1 & 1998 & APPLE INC & MICROSOFT CORP & 0.0601 & 36 & 73 & 71.79 & 517.38 \\
  2 & 1998 & APPLE INC & ORACLE CORP & 0.0355 & 36 & 73 & 71.79 & 188.44 \\
  3 & 1998 & AT\&T INC & US CELLULAR CORP & 0.0761 & 48 & 48 & 324.14 & 57.62 \\
  4 & 1998 & EBAY INC & MICROSOFT CORP & 0.0281 & 73 & 73 & 98.54 & 517.38 \\
  \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
  437 & 2017 & ORACLE CORP & MICROSOFT CORP & 0.1292 & 73 & 73 & 432.13 & 728.91 \\
  438 & 2017 & ORACLE CORP & INTUIT INC & 0.0231 & 73 & 73 & 432.13 & 187.30 \\
  439 & 2017 & US CELLULAR CORP & AT\&T INC & 0.0184 & 48 & 48 & 56.56 & 488.57 \\
  440 & 2017 & WESTERN DIGITAL CORP & APPLE INC & 0.0321 & 35 & 36 & 161.40 & 888.85 \\
  441 & 2017 & WESTERN DIGITAL CORP & MICRON TECHNOLOGY INC & 0.0788 & 35 & 36 & 161.40 & 188.55 \\
  \hline
  \end{tabular}
  \caption{Overview of the TNIC Sample Data} \label{tab:data-overview}
\end{table}
  


\subsection{Data Preparation}

\pkg{evomap} is designed to analyze longitudinal relationship data. Specifically, the input data should consist
of a sequence of $n \times n$ relationship matrices containing non-negative real numbers, where $n \in \mathbb{N}$
denotes the number of objects. For most applications, these relationships contain either dissimilarities
or similarities, are symmetric, normalized to a common scale (e.g., between 0 and 1), and do not include
missing values. In general, data need to be balanced, meaning the set of objects should remain timeinvariant.
The later section XXXXX shows how to handle unbalanced data as well.

Most mapping methods, like MDS, typically require dissimilarities as their input. In cases
where a different type of data is available, users can translate their data into the required type using
the functions provided in \texttt{evomap.preprocessing}. This submodule also provides functions to
translate related formats, such as edgelists or cooccurrence matrices, into the required format.

In the TNIC example, the sample consists of a time-indexed edgelist, rather than a sequence of
matrices. Further, the sample consists of measures of similarity, rather than dissimilarity. Therefore,
as a first step, we transform the edgelist into a sequence of matrices, using the \code{edgelist2matrices()}
function. When applying the function, we specify the relevant columns in the data that indicate the
objects, their similarity score, and the corresponding points in time.

\begin{Code}
>>> from evomap.preprocessing import edgelist2matrices
>>> S_t, labels_t = edgelist2matrices(
      data,
      score_var = 'score',
      id_var_i = 'name1',
      id_var_j = 'name2',
      time_var = 'year')
\end{Code}

Second, we translate similarities into dissimilarities, using the simple transformation 
$diss_{i,j} = 1 - sim_{i,j}$, available in \code{sim2diss()}. We call the function for each similarity matrix and store the
results in a new list:

\begin{Code}
>>> from evomap.preprocessing import sim2diss
>>> D_t = []
>>> for S in S_t:
        D_t.append(sim2diss(S, transformation = 'mirror'))
\end{Code}
  
As a result, we obtain a list of 20 dissimilarity matrices of shape (9,9), which we can use as
the input for EvoMap.

\begin{CodeChunk}
  \begin{CodeInput}
    >>> D_t[0].round(2)
  \end{CodeInput}
  \begin{CodeOutput}
  array([[0. , 1. , 1. , 1. , 1. , 0.94, 0.96, 1. , 0.93],
      [1. , 0. , 1. , 1. , 1. , 1. , 1. , 0.92, 1. ],
      [1. , 1. , 0. , 0.91, 1. , 0.97, 1. , 1. , 1. ],
      [1. , 1. , 0.91, 0. , 1. , 0.95, 1. , 1. , 1. ],
      [1. , 1. , 1. , 1. , 0. , 1. , 1. , 1. , 0.97],
      [0.94, 1. , 0.97, 0.95, 1. , 0. , 0.9 , 1. , 0.97],
      [0.96, 1. , 1. , 1. , 1. , 0.9 , 0. , 1. , 0.97],
      [1. , 0.92, 1. , 1. , 1. , 1. , 1. , 0. , 1. ],
      [0.93, 1. , 1. , 1. , 0.97, 0.97, 0.97, 1. , 0. ]])
  \end{CodeOutput}
\end{CodeChunk}

\subsection{Dynamic Mapping}

\subsubsection{Basic Syntax}

As seen before, fitting EvoMap to data requires to instantiate the method and calling \code{fit\_transform()}.

\begin{CodeChunk}
  \begin{CodeInput}
>>> from evomap.mapping import EvoMDS
>>> evomds = EvoMDS()
>>> X_t = evomds.fit_transform(D_t)
  \end{CodeInput}
\end{CodeChunk}

When instantiating the method (here: EvoMDS), the user has various options to control the output
generation. The two most important considerations are: setting EvoMap‚Äôs hyperparameters ($\alpha$ and
$p$) and setting other method-specific arguments.

For EvoMap‚Äôs hyperparameters, the default values are $\alpha = 0$ and $p = 1$, corresponding to fully
independent mapping. Recall that $\alpha$ controls how strongly EvoMap aligns subsequent maps, while
$p$ controls the degree of smoothing in objects‚Äô movement paths. Both profoundly impact the
solution. If $\alpha$ is too low, for instance, each map may fit its corresponding data well, but subsequent
maps will tend to be misaligned. Conversely, if $\alpha$ is too high, subsequent maps may be strongly
aligned, but the quality of individual maps may suffer. Section XXXXXX explores setting EvoMap‚Äôs
hyperparameters in more detail.

For the TNIC example, we use an $\alpha$ value of 0.2 and the default value of $p = 1$. As we intend
to use EvoMap with non-metric MDS, we set the MDS-specific argument \code{mds\_type} to ‚Äúordinal‚Äù.

\begin{CodeChunk}
  \begin{CodeInput}
    >>> from evomap.mapping import EvoMDS
    >>> evomds = EvoMDS(alpha = .2, mds_type = 'ordinal')
    >>> X_t = evomds.fit_transform(D_t)  
\end{CodeInput}
\end{CodeChunk}

To explore the results, we can use \code{draw\_map()} from the printer submodule, which plots a
single snapshot. Here, we use \code{draw\_map()} to plot the first and last snapshot in the created sequence,
adding each firm‚Äôs name as a label next to each point, and showing the, by default hidden, axes.

\begin{CodeChunk}
  \begin{CodeInput}
    >>> from evomap.printer import draw_map
    >>> draw_map(X_t[0], label = labels, show_axes = True)
    >>> draw_map(X_t[-1], label = labels, show_axes = True)
\end{CodeInput}
\end{CodeChunk}

\begin{figure}[t!]
  \centering
  \includegraphics{../gen/sect4_evomds_snapshots.png}
  \caption{\label{fig:fig:evomds-snapshots} EvoMap Snapshots for 1998 and 2017.
  from school.}
  \end{figure}
  
This side-by-side comparison provides a first glance at EvoMap‚Äôs output. The overall map
remains relatively stable, which allows us to compare the positions across the two snapshots. Some
firms, such as Apple, exhibit notable changes in their positions. Microsoft and Oracle appear to
move closer, as do Apple and Intuit, while Apple‚Äôs distance from Western Digital clearly increases.
Naturally, such a side-by-side comparison only allows us to inspect the results simultaneously at
two points in time, which might obscure essential insights into when and how these changes occur,
and for how long they persist. Other options, detailed in the following sections, provide these
insights by displaying the entire sequence of maps in one graph.

While the example presented thus far already suffices to apply EvoMap, there are two additional
aspects the user should always consider when using EvoMap: setting fixed starting positions and
inspecting convergence diagnostics.

\subsection{Starting Positions}

By default, the optimization routine is randomly initialized each time the user calls \code{fit()} or
\code{fit_transform()}, leading to slightly different results upon multiple runs. To ensure reproducibility, the
user can either set a fixed seed for the random number generator or initialize the optimization with fixed
starting positions. The former requires calling \code{np.random.seed()} with a fixed number before running
EvoMap. The latter requires setting the init argument when instantiating the method.

The \code{init} argument takes a list of positions, all having the same shape as the output positions
(here: (9,2)). Two natural choices for the starting positions are either random positions or the
solution of another mapping method. When using MDS, a popular choice is to first apply Classical
Scaling and use the solution as starting positions, which can help find a lower Stress solution (see,
for instance, \citep{DeLeeuw+Mair:2009}.

For the remainder of the TNIC example, we use the Classical Scaling solution of the first period
as fixed starting positions. Classical Scaling is entirely deterministic, independent of setting random
seeds, and thus fully reproducible. Classical Scaling is implemented in the texttt{evomap.mapping.CMDS}
class that the user can apply similarly to all other mapping methods using \code{fit_transform()}. We
create a list of starting positions, each containing the Classical Scaling solution, and fit EvoMap
setting the init argument.

\begin{CodeChunk}
  \begin{CodeInput}
>>> from evomap.mapping import CMDS
>>> cmds_t = []
>>> cmds = CMDS().fit_transform(D_t[0])
>>> for t in range(n_periods):
        cmds_t.append(cmds)
>>> evomds_cmds = EvoMDS(
                    alpha = .2,
                    mds_type = 'ordinal',
                    init = cmds_t)
>>> X_t_cmds = evomds_cmds.fit_transform(D_t)
\end{CodeInput}
\end{CodeChunk}

\begin{figure}[t!]
  \centering
  \includegraphics{../gen/sect4_evomds_cmds_snapshots.png}
  \caption{\label{fig:evomds-snapshots-cmds} EvoMap Snapshots for 1998 and 2017, Initialized by Classical Scaling.}
  \end{figure}

\subsection{Convergence Diagnostics}

By default, \pkg{evomap} generates its output silently, providing no information about the gradient method‚Äôs
convergence. In some cases, the user might require such information as when the solution quality
appears poor, or when results are otherwise unexpected. In such cases, inspecting convergence
diagnostics allows the user to ensure that the depicted solution is not the result of poor optimization
performance. To obtain this information, the user can adjust the verbosity level by setting the verbose
argument when instantiating the method.

By default, verbose is set to 0, suppressing all output. Increasing verbose to 1 provides more
information about the type of optimization routine, the final cost function value, and an indication
of whether the optimization process converged or reached the maximum number of iterations.
Setting verbose to 2 additionally allows tracking convergence by displaying the current cost
function value at fixed intervals during optimization.

In the TNIC example, setting verbose = 0 yields no output.

\begin{CodeChunk}
  \begin{CodeInput}
>>> EvoMDS(
  alpha = .2,
  mds_type = 'ordinal',
  init = cmds_t,
  verbose = 0).fit(D_t)
\end{CodeInput}
\end{CodeChunk}

Setting \code{verbose = 1} indicates the optimization routine (gradient descent with backtracking)
and an indication of its successful convergence at iteration 105:

\begin{CodeChunk}
  \begin{CodeInput}
>>> EvoMDS(
  alpha = .2,
  mds_type = 'ordinal',
  init = cmds_t,
  verbose = 1).fit(D_t)
\end{CodeInput}
\begin{CodeOutput}
[EvoMDS] Running Gradient Descent with Backtracking via Halving
[EvoMDS] Iteration 105: gradient norm vanished. Final cost: 3.80
\end{CodeOutput}
\end{CodeChunk}

Setting \code{verbose = 2} shows more details on the convergence of the gradient method. Checking
the cost function every 20 iterations reveals how the cost function values decrease monotonically,
and the gradient norm diminishes until it vanishes around iteration 105.

\begin{CodeChunk}
  \begin{CodeInput}
>>> EvoMDS(
  alpha = .2,
  mds_type = 'ordinal',
  init = cmds_t,
  n_iter_check = 20,
  verbose = 2).fit(D_t)
\end{CodeInput}
\begin{CodeOutput}
[EvoMDS] Running Gradient Descent with Backtracking via Halving
[EvoMDS] Iteration 20 -- Cost: 3.81 -- Gradient Norm: 0.0245
[EvoMDS] Iteration 40 -- Cost: 3.80 -- Gradient Norm: 0.0104
[EvoMDS] Iteration 60 -- Cost: 3.80 -- Gradient Norm: 0.0076
[EvoMDS] Iteration 80 -- Cost: 3.80 -- Gradient Norm: 0.0058
[EvoMDS] Iteration 100 -- Cost: 3.80 -- Gradient Norm: 0.0012
[EvoMDS] Iteration 105: gradient norm vanished. Final cost: 3.80
\end{CodeOutput}
\end{CodeChunk}

Here, these diagnostics indicate good convergence of the optimization. In cases where this
output should suggest otherwise, different options are available to ensure proper convergence of
the gradient method.

\begin{itemize}
  \item \code{evomap.preprocessing} allows normalizing dissimilarity matrices prior to mapping, as some
  methods may struggle with input data measured on unusual or very large scales.
  \item If the optimization process does not converge within the default number of iterations (by
  default: 2000), users can increase the value of \code{n_iter} to allow for more iterations.
  \item Alternatively, increasing the step sizes by adjusting the \code{step_size} argument (by default: 1)
  can aid quicker convergence. Conversely, if cost values increase during optimization, it may
  indicate that step sizes are too large. In such cases, users can reduce \code{step_size} or increase
  \code{max_halves} (by default: 5) when backtracking is used.
  \item If the user suspects that results are strongly influenced by an unlucky draw of random
  starting positions, setting \code{n_inits} to a higher value (by default: 1) will run the method
  multiple times using different random initializations and return the solution that best fits the
  data. Doing so can also be a good way to test the results‚Äô sensitivity to different starting
  positions.
\end{itemize}

Once the user is confident about proper convergence of the optimization, one can start exploring
the results in more depth and evaluate the solution quality more rigorously.

\subsection{Exploration}

The evomap package provides a range of options to explore the results graphically through the printer
submodule. One can categorize these functions into two groups:

\begin{enumerate}
  \item Instantiating EvoMap for a chosen mapping method,
  \item Fitting EvoMap to the data.
\end{enumerate}

\subsubsection{Static Maps}

As seen in the previous section, inspecting the results at individual points in time can help evaluate the
solution's quality, detect patterns, and establish reference points for further comparison. Therefore,
creating static snapshots is always a helpful first step in exploring EvoMap‚Äôs results. Subsequently, the
evolution of the maps can be explored more thoroughly through dynamic visualizations.

The core function for exploring results in static snapshots is \code{draw_map()}. In its basic form,
\code{draw_map()} takes a single argument, a map $\hat{X}$, and visualizes it in a scatter plot. If $\hat{X}$ is
unidimensional, it plots a single scale instead. The user can extend this most basic map by providing
additional information about each object and binding them to different aesthetics of the map. The
\code{label} argument, for instance, allows adding a label for each object from a list of strings; the \code{color}
argument allows coloring each point based on a discrete set of values. Additionally, the \code{size}
argument allows adjusting the size of each point to a list of continuous variables.

In the TNIC example, we first create two lists containing further information about the firms
depicted on the map: their industry membership, measured as their two-digit SIC code, and their
size, measured by a monotonous transformation of their market value. We retrieve the respective
information for each firm. Then, we provide these lists as additional arguments to \code{draw_map()}.

\begin{CodeChunk}
  \begin{CodeInput}
>>> sic_codes = []
>>> for firm in labels:
        sic_codes.append(data.query('name1 == @firm').sic1.unique()[0])
>>> print(sic_codes)
\end{CodeInput}
\begin{CodeOutput}
[36, 48, 73, 73, 36, 73, 73, 48, 35]
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
>>> sizes = []
>>> for firm in labels:
        sizes.append(data.query('name1 == @firm').size1.unique()[0].round(2)
>>> print(sizes)
\end{CodeInput}
  \begin{CodeOutput}
[71.79, 324.14, 98.54, 54.32, 69.69, 517.38, 188.44, 57.62, 32.29]
  \end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
>>> draw_map(X_t[0])
>>> draw_map(X_t[0], label = labels, title = periods[0])
>>> draw_map(X_t[0], label = labels, color = sic_codes, title = periods[0])
>>> draw_map(X_t[0], label = labels, color = sic_codes, size = sizes, title = periods[0])
  \end{CodeInput}
\end{CodeChunk}

\begin{figure}[t!]
  \centering
  \includegraphics{../gen/sect4_draw_map_examples.png}
  \caption{\label{fig:draw-map} A Single Static Snapshot Drawn With Different Aesthetics.}
  \end{figure}

The added aesthetics reveal more insights into the data structure and its potential drivers. Here,
for instance, firms‚Äô positions tend to agree with their two-digit SIC assignments, indicated by the
close placement of firms within the same SIC code (here: red and green firms).
The natural extension of drawing a single snapshot is drawing multiple snapshots. Such sideby-
side comparisons of multiple snapshots can represent the next step to understanding how and
when positions change from one period to another. One can create such a sequence manually,
calling \code{draw_map()} multiple times, or automatically, using \code{draw_map_sequence()}. Here, we use
\code{draw_map_sequence()} to track the map‚Äôs evolution over the first four years between 1998 and 2001.
Note how the additional arguments used previously with \code{draw_map()} (\code{color}, \code{label}, \code{size}) can also
be passed to \code{draw_map_sequence()}. Figure \ref{fig:draw-map-sequence} displays the results

\begin{CodeChunk}
  \begin{CodeInput}
>>> from evomap.printer import draw_map_sequence
>>> draw_map_sequence(X_t[:4],
        color = sic_codes,
        label = labels,
        size = sizes,
        time_labels = periods[:4])
  \end{CodeInput}
  \end{CodeChunk}

  \begin{figure}[t!]
    \centering
    \includegraphics{../gen/sect4_draw_map_sequence.png}
    \caption{\label{fig:draw-map-sequence} Sequence of Four Subsequent EvoMap Snapshots.}
    \end{figure}

In the TNIC example, the sequence of the first four snapshots indicates how Apple slowly drifts
away from its former peers of hardware firms, moving closer to the platform and software-focused
firms below. Likewise, Intuit slowly moves away from eBay, heading in a similar direction as
Apple.

\subsubsection{Dynamic Maps}

In addition to examining individual snapshots in side-by-side comparisons, users can gain deeper
insights into the maps‚Äô evolution by using overlays of multiple subsequent snapshots. To create such
dynamic maps, \pkg{evomap} offers two related functions: \code{draw_dynamic_map()} and \code{draw_trajectories()}.

The \code{draw_dynamic_map()} function creates an overlay of individual snapshots generated by
\code{draw_map()}. Therefore, the dynamic map can utilize the same additional aesthetics as \code{draw_map()},
but also displays how positions change over time. Moreover, the added aesthetics, such as color or
size, can change over time as well. Accordingly, the required input to bind a variable to the color
of each map is no longer a single array. Instead, the required input is a list of arrays, where each
array contains the respective values for a single point in time. Similarly, to adjust the size of each
point based on a continuous variable, the function expects a list of arrays, each containing the
respective variable values at a single point in time.

In the TNIC example, we use colors to represent sic codes and size to indicate market value.
To achieve this, at each point in time we create individual arrays containing the sic codes and size
values for each firm. We then combine these arrays in a list and provide the resultant lists as
additional arguments to \code{draw_dynamic_map()}. Additionally, we set \code{show_arrows} to \code{True}, which
displays lines connecting each object's subsequent positions on the map. The left part of Figure \ref{fig:dynamic-map}
displays the resulting dynamic map. Further, we use the default view generated by
\code{draw_trajectories()}, which focuses on the movement paths of individual objects. The right part
of Figure \ref{fig:dynamic-map} displays the resultant trajectories.

\begin{CodeChunk}
  \begin{CodeInput}
>>> # Create input for map aesthetics
>>> sic_codes_t = []
>>> sizes_t = []
>>> for t in range(n_periods):
>>>     # get data in period t
>>>     data_this = data.query('year == @periods[@t]')
>>>     # get all sic codes in period t
>>>     sic_this = []
>>>     for firm in labels:
>>>         sic_this.append(data_this.query('name1 == @firm').sic1.unique()[0])
>>>         sic_codes_t.append(np.array(sic_this))
>>>     sizes_this = []
>>>     for firm in labels:
>>>         sizes_this.append(data_this.query('name1 == @firm').size1.unique()[0])
>>>         sizes_t.append(np.array(sizes_this))
  \end{CodeInput}
  \end{CodeChunk}

  \begin{CodeChunk}
    \begin{CodeInput}
>>> # Draw maps
>>> from evomap.printer import draw_dynamic_map
>>> draw_dynamic_map(X_t,
        label = labels,
        color_t = sic_codes_t,
        size_t = sizes_t,
        show_arrows = True,
        title = 'A: Dynamic Map')
    \end{CodeInput}
    \end{CodeChunk}

    \begin{CodeChunk}
      \begin{CodeInput}
>>> from evomap.printer import draw_trajectories
>>> draw_trajectories(X_t,
        labels = labels,
        period_labels = periods,
        title = "B: Trajectories")
      \end{CodeInput}
      \end{CodeChunk}

\begin{figure}[t!]
\centering
\includegraphics{../gen/sect4_dynamic_map_and_trajectories.png}
\caption{\label{fig:draw-dynamic-map} Dynamic Maps for TNIC Sample, Generated by EvoMap.}
\end{figure}

As seen in Figure \ref{fig:draw-dynamic-map}, the maps are well-aligned, but the resultant trajectories are still somewhat
erratic, which impairs clear interpretation. We therefore add some degree of smoothing by
additionally increasing the hyperparameter $p$ to 2 (default: 1, corresponding to no smoothing).
Figure \ref{fig:draw-dynamic-map-with-smoothing} displays the results, which is the solution presented earlier when illustrating dynamic
mapping in section XXXXX.

\begin{figure}[t!]
\centering
\includegraphics{../gen/sect4_dynamic_map_and_trajectories_p2.png}
\caption{\label{fig:draw-dynamic-map-with-smoothing} Dynamic Maps for TNIC Sample, Generated by EvoMap with Smoothing.}
\end{figure}

\begin{CodeChunk}
\begin{CodeInput}
>>> evomds = EvoMDS(alpha = .2, p = 2, mds_type = 'ordinal', init = cmds_t)
>>> X_t = evomds.fit_transform(D_t)
\end{CodeInput}
\end{CodeChunk}

The new Figure, including both alignment and smoothing, reveals the trajectories of the nine
firms more clearly. Focusing on Apple, for instance, we see how Apple and Western Digital quickly
part ways, as Apple is moving away from the hardware-focused firms, hovering closer to the
platform-focused firms, where it converges with software-focused firms, like Intuit.

\subsection{Evaluation}

Beyond mere visual evaluation, the user can also evaluate all results quantitatively. To do so, the user
can either inspect the values of the cost function or use dedicated evaluation metrics available in the
metrics submodule. Inspecting the values of the cost function, such as Stress, is particularly useful to
assess the solution quality at individual points in time, or when comparing the results obtained from the
same mapping method under different hyperparameters, starting configurations, or optimization
settings. When comparing results across methods, these values become less informative.

In the TNIC example, we first use the static cost function values to assess how strongly the
alignment via EvoMap impairs the quality of each individual mapping solution. To do so, we
compare the average Stress of our solution against the average Stress of an entirely independently
generated sequence. As seen in the output, the increase in Stress is marginal at roughly .011.

\begin{CodeChunk}
\begin{CodeInput}
>>> evomds_indep = EvoMDS(alpha = 0, init = cmds_t, mds_type= 'ordinal').fit(D_t)
>>> print(evomds_indep.cost_static_avg_.round(3))
\end{CodeInput}
\begin{CodeOutput}
0.180
\end{CodeOutput}
\end{CodeChunk}

\begin{CodeChunk}
\begin{CodeInput}
>>> print(evomds.cost_static_avg_.round(3))
\end{CodeInput}
\begin{CodeOutput}
0.191
\end{CodeOutput}
\end{CodeChunk}

Table \ref{tab:metrics-overview} lists the additional evaluation metrics implemented in the metrics submodule. The
Table lists the metric, its underlying intuition, whether it is designed to evaluate a single snapshot
or entire sequence, and the range of possible values. We refer to \cite{Matthe+Ringel+Skiera:2023} for more
details on these metrics and their underlying intuition.

\begin{table}[t!]
    \centering
    \begin{tabular}{p{3cm}p{4cm}p{3cm}p{2cm}}
      \hline
      Metric & Intuition & Snapshot vs. Sequence & Range \\
      \hline
      Misalignment & Total length of all movement paths & Sequence & 0 \(good\) to Inf \(poor\) \\
      Alignment   & Average cosine similarity of positions & Sequence & -1 \(poor\) to 1 \(good\) \\
      Hit-Rate & Agreement in nearest neighbors between data and the map. & Snapshot & 0 \(poor\) to 1 \(good\) \\ 
      Adjusted Hit-Rate & Hit-Rate, adjusted for random agreement. & Snapshot & 0 \(poor\) to 1 \(good\) \\
      Avg. Hit-Rate & Average Hit-Rate over multiple periods. & Sequence & 0 \(poor\) to 1 \(good\) \\
      Avg. Adj. Hit-Rate & Average adjusted Hit-Rate over multiple periods. & Sequence & 0 \(poor\) to 1 \(good\) \\
      Persistence & Correlation of subsequent movement vectors on the map. & Sequence & -1 \(poor\) to 1 \(good\) \\
      \hline
  \end{tabular}
    \caption{Overview of Available Evaluation Metrics}
    \label{tab:metrics-overview}
\end{table}

In the TNIC example, we use the following metrics: alignment, persistence, and average Hit-
Rate. We compute each metric for each of the three alternatives depicted in section XXX: fully
independent mapping, independent mapping with ex-post alignment, and EvoMap. Table \ref{tab:metrics-comparison}
displays the results, which quantitatively confirm the visual perception of the resultant dynamic
maps: EvoMap‚Äôs solution is better aligned, the movement paths are smoother and easier to interpret,
and the static mapping quality is not significantly reduced.

\begin{table}[t!]
  \centering
  \begin{tabular}{lllll}
    \hline
    Method & Misalignment & Persistence & Avg. Hit-Rate & Avg. Stress \\
    \hline
    Indep. MDS & 1.1694 & -0.6240 & 0.6944 & 0.1802 \\
    Indep. MDS + Alignment & 0.1675 & -0.4653 & 0.7069 & 0.1802 \\
    EvoMDS & 0.0497 & 0.6405 & 0.7069 & 0.1910 \\
    \hline
\end{tabular}
  \caption{Evaluation of Mapping Solutions in Figure \ref{fig:dynamic-map}}
  \label{tab:metrics-comparison}
\end{table}

\begin{CodeChunk}
  \begin{CodeInput}
>>> # Compute evaluation metrics
>>> from evomap.metrics import *
>>> misalign_score_t = []
>>> persistence_score_t = []
>>> avg_hitrate_score_t = []|
>>> for i, X in enumerate([X_t, X_t_indep, X_t_indep_aligned]):
        misalign_score_t.append(misalign_score(X))
        persistence_score_t.append(persistence_score(X))
        avg_hitrate_score_t.append(avg_hitrate_score(X, D_t, input_format = 'dissimilarity', n_neighbors = 4))

>>> # Compute evaluation metrics
>>> metrics = pd.DataFrame({'misalign_score': misalign_score_t,
                            'persistence_score': persistence_score_t,
                            'hitrate_score': avg_hitrate_score_t}, 
                            index = ['EvoMDS', 'Independent MDS', 'Independent MDS + Alignment'])

>>> metrics['average_stress'] = [
                            evomds_cmds.cost_static_avg_,
                            evomds_indep.cost_static_avg_,
                            evomds_indep.cost_static_avg_]
>>> metrics = metrics.reindex([
                            'Independent MDS',
                            'Independent MDS + Alignment',
                            'EvoMDS'])
\end{CodeInput}
\end{CodeChunk}
  
\section{Advanced Features} \label{sec:advanced}

To close the presentation of the \pkg{evomap} package, this final section showcases two additional
considerations that are relevant in many applications: finding the right hyperparameter values and
dealing with unbalanced data.

\subsection{Setting Hyperparameters}

EvoMap requires the user to set two key hyperparameters in any dynamic mapping application: $\alpha$ and
$p$. $\alpha$ controls the alignment of subsequent snapshots, whereas $p$ controls the resultant movement paths‚Äô
smoothness over multiple periods. Both hyperparameters have a profound impact on EvoMap‚Äôs output.
Therefore, each dynamic mapping application requires the user to carefully identify suitable values for
them. Within the \pkg{evomap} package, the user has two ways to do so: visually inspecting individual
hyperparameter combinations or quantitively evaluating a broader range of combinations in a grid
search.

To illustrate the hyperparameters‚Äô impact on the result, we visually inspect EvoMap‚Äôs output
under different hyperparameter values, fitting three instances of EvoMap for different values for $\alpha$
and $p$, respectively. Figure \ref{fig:hyperparameters} shows the results, which illustrate the hyperparameters‚Äô influence
for the TNIC sample.

\begin{figure}[t!]
  \centering
  \includegraphics{../gen/sect5_hyperparamters.png}
  \caption{\label{fig:hyperparameters} Dynamic Map Under Different Hyperparameter Choices.}
  \end{figure}

As seen in Figure 3 \ref{fig:hyperparameters}, a low value for Œ± results in weak alignment and somewhat erratic
movement paths. Increasing Œ± gradually increases alignment and decreases the total length of
objects‚Äô movement paths. When Œ± is set (too) high, the resultant map remains almost entirely static.
The hyperparameter ùëù, in contrast, has little effect on the total length of the movement paths.

\subsection{Dealing with Unbalanced Data}
